<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

#LASSO
######王君銘 吴艳辉

## 简单线性回归
在做统计分析时，为了解释变量之间的关系，我们最常用的方法是假设解释变量和被解释变量线性相关，即$y=\beta^Tx$。为了取得合适的$\widehat\beta$，使得$y$与$\widehat y$之间的差距尽可能的小，通常情况下我们会使用最小二乘法求解最优解，即通过最小化误差平方和来求得$\widehat\beta$的值

$$\widehat\beta=\arg \min _{\beta} \sum_{i=1}^{N}\left(y_{i}-\beta^{T} x_{i}\right)=\left(X^{T} X\right)^{-1} X^{T}y$$

在使用最小二乘法时通常要求样本数要大于变量数，即要求$N>p$。从上式可看到该方法的求解过程需要矩阵的逆，如果$p>N$导致其协方差矩阵不可逆。这时目标函数最小化导数为零时，方程有无数解，这样就没有办法求出最优解，这会导致模型的过度拟合。

## 正则化
为了防止模型的过度拟合，我们需要对其进行正则化，即给相应的目标函数做出一些限制，使得其最优解的空间变小。$$\widehat{\beta}=\arg \min _{\beta} \sum_{i=1}^{N}\left(y_{i}-\beta^{T} x_{i}\right)+P_{\lambda}(|\beta|)$$$$P_{\lambda}(|\beta|)=\lambda \sum_{j=1}^{d}\left|\beta_{j}\right|^{m}$$上式中的$P_{\lambda}(|\beta|)$即为相应的限制，其中$m$的取值决定了限制的范围。当$m=1$时，即为Lasso回归，当$m=2$时，即为Ridge回归.


##Lasso
Lasso是由1996年由Robert Tibshirani提出的，全称为Least absolute shrinkage and selection operator。$$\widehat{\beta}^{L}=\arg \min _{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}$$该方法通过在残差平方和最小化的计算中加入$l_1$loss作为约束，得到一个较精炼的模型,该模型能够压缩一些回归系数，使得回归系数的绝对值之和小于某个固定值，并且在$\lambda$充分大时,可以把某些回归系数精确地收缩到零。该模型保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。
<center>
<img src="/Users/as/Desktop/lasso.png" width="50%" height="50%" alt="lasso" align=center>
</center>

可以看到lasso的约束域为正方形，因此其等高线可能存在与坐标轴的切点，使得部分维度的特征权重为0，从而达到变量选择的效果，将不具有显著性的变量的系数压缩到0，从而将不显著的变量舍弃。


##LASSO应用

1. 防止模型的回归系数过大，造成过度拟合。
2. 适用于高维统计。高维数据是指数据的维数很高，远大于样本量，其明显表现是，相对于空间维数，样本量显得非常稀疏。对于高维数据带来的过度拟合问题，其解决思路为一是增加样本样，二是减少样本特征。但是现实世界获取的样本量有限，因此只能通过减少样本特征对模型进行降维。因为lasso可以使部分变量系数变为0，可以运用lasso对模型进行特征选择，减少样本特征，从而实现降维降维。
3. 将lasso应用于回归，可以在参数估计的同时实现变量的选择，较好的解决回归分析中的多重共线性的。当$\lambda$足够大时，可以通过lasso进行变量选择，减少模型的复杂度，产生稀疏模型（sparse model)，即只有一些变量子集的模型。

## LASSO的python实现
Lasso在python中主要通过slearn库中的linear_model中的lasso代码进行实现。
```python
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso,LassoCV,LassoLarsCV  

```


```python
# 样本数据集，第一列为x，第二列为y，在x和y之间建立回归模型
data=np.random.randn(30,2)
#随机生成X和y矩阵
dataMat = np.array(data)
X = dataMat[:,0:1]   
y = dataMat[:,1]   
```


```python
model = Lasso(alpha=0.01)  # 调节alpha可以实现对拟合的程度

model.fit(X, y)   # 线性回归建模
print('系数矩阵:\n',model.coef_)
print('线性回归模型:\n',model)

predicted = model.predict(X)

# 绘图
plt.scatter(X, y, marker='x')
plt.plot(X, predicted,c='r')

plt.xlabel("x")
plt.ylabel("y")

plt.show()

```

    系数矩阵:
     [0.29262326]
    线性回归模型:
     Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,
       normalize=False, positive=False, precompute=False, random_state=None,
       selection='cyclic', tol=0.0001, warm_start=False)


![png](output_2_1.png)


```python

```

##参考文献
[1][Lasso回归](https://blog.csdn.net/xiaozhu_1024/article/details/80585151)
[2][Model_Selection_and_Regularization](https://jiamingmao.github.io/data-analysis/assets/Lectures/Model_Selection_and_Regularization.pdf)
[3][Lasso regression(稀疏学习,R)](https://blog.csdn.net/hfutxiaoguozhi/article/details/78847040)
[4][什么是正则化](https://blog.csdn.net/haima1998/article/details/79425831)
